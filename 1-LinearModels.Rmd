---
title: "ETC3580: Advanced Statistical Modelling"
author: "Week 1: Linear models"
fontsize: 14pt
output:
  beamer_presentation:
    theme: metropolis
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE,
  dev.args=list(bg=grey(0.9), pointsize=11))
library(fpp2)
```

# Linear Models in R

## Linear Models
\fontsize{13}{13}\sf
\begin{block}{}\centering
$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_{p-1}x_{p-1} + \varepsilon$
\end{block}

 * Response: $y$
 * Predictors: $x_1,\dots,x_{p-1}$
 * Error: $\varepsilon$

Let $\bm{y} = (y_1,\dots,y_n)'$, $\bm{\varepsilon} = (\varepsilon_1,\dots,\varepsilon_n)'$, $\bm{\beta} = (\beta_0,\dots,\beta_{p-1})'$ and
\[
\bm{X} = \begin{bmatrix}
1      & x_{1,1} & x_{2,1} & \dots & x_{p-1,1}\\
1      & x_{1,2} & x_{2,2} & \dots & x_{p-1,2}\\
\vdots & \vdots  & \vdots  &       & \vdots\\
1      & x_{1,n} & x_{2,n} & \dots & x_{p-1,n}
  \end{bmatrix} \qquad \text{(the model matrix).}
\]\pause

Then

###
\centering
$\bm{y} = \bm{X}\bm{\beta} + \bm{\varepsilon}.$

## Matrix formulation

**Least squares estimation**

Minimize: $(\bm{y} - \bm{X}\bm{\beta})'(\bm{y} - \bm{X}\bm{\beta})$\pause

Differentiate wrt $\bm{\beta}$ gives

\begin{block}{The ``normal'' equations}
\[
\hat{\bm{\beta}} = (\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y}
\]
\end{block}

## Likelihood

If the errors are iid and normally distributed, then
\[
\bm{y} \sim \text{N}(\bm{X}\bm{\beta},\sigma^2\bm{I}).
\]\pause
So the likelihood is
\[
L = \frac{1}{\sigma^T(2\pi)^{T/2}}\exp\left(-\frac1{2\sigma^2}(\bm{y}-\bm{X}\bm{\beta})'(\bm{y}-\bm{X}\bm{\beta})\right)
\]\pause
which is maximized when $(\bm{y}-\bm{X}\bm{\beta})'(\bm{y}-\bm{X}\bm{\beta})$ is minimized.\pause

\centerline{\textcolor{orange}{So \textbf{MLE = OLS}.}}

## R modelling notation

```r
fit <- lm(response ~ x1 + x2 + x3,
  data=tibble)
```

\begin{block}{}
$$\text{\tt response} = \beta_0 + \beta_1 \text{\tt x1}
 + \beta_2 \text{\tt x2}
 + \beta_3 \text{\tt x3}
 + \varepsilon$$
\end{block}

## Useful helper functions
 * `summary`
 * `coef`
 * `fitted`
 * `predict`
 * `residuals`
 * `deviance`
 * `df.residual`
 * `plot`

## R formulas

\fontsize{13}{14}\sf

\alert{Categorical predictors}:

 * R will create the required dummy variables from a categorical *factor*.
 * The first level is used as the reference category.
 * Use `relevel` to change the reference category

\fontsize{12}{12}\sf

| Expression | Description |
| :---------- |:-----------|
| `y ~ x` | Simple regression|
| `y ~ 1 + x` | Explicit intercept|
| `y ~ -1 + x`  | Through the origin|
| `y ~ x + I(x^2)`  | Quadratic regression|
| `y ~ x1 + x2 + x3`  | Multiple regression|
| `sqrt(y) ~ x + I(x^2)`  | Transformed|
| `y ~ . -x1` | All variables except x1|

## Interactions

\fontsize{13}{14}\sf
\alert{Interactions}:

 * Interactions are obtained by multiplying the relevant columns of the model matrix.
 * Use `a:b` for the interaction between `a` and `b`.
 * Use `a*b` to mean `a + b + a:b`

\alert{Limited order interactions}:

  * Interactions up to 2nd order can be specified using the `^` operator.
  * `(a+b+c)^2` is identical to `(a+b+c)*(a+b+c)`

\alert{Nested factors}:

 * `a + b %in% a`   expands to `a + a:b`.

## Interpretation

* Each coefficient gives the effect of a one unit increase of the predictor on the response variable, *holding all other variables constant*.
* Be careful with interactions: you cannot interpret the main effects when there is an interaction between them.

## Hypothesis testing

* Use F-tests between models:

    -  Model 1: $p_1$ parameters.
    -  Model 2 (nested within Model 1): $p_2$ parameters

\begin{block}{}
$$F = \frac{(\text{RSS}_2 - \text{RSS}_1)/(p_1-p_2)}
           {\text{RSS}_1/(n-p_1)}
    \sim F_{p_1-p_2, n-p_1}
$$
\end{block}

* Helper functions: `anova`, `drop1`
* If one term dropped, this is equivalent to a t-test on coefficient.

# Regression diagnostics

## Regression diagnostics

 * Residuals vs Fitted: Check heteroskedasticity
 * Scale-Location: standardized residuals vs fitted values.
 * Normal QQ plot: Check for non-normality
 * Residuals vs Leverage: Check for influential points

Produced using `autoplot` or `plot`

## LOO Residuals

\begin{block}{Fitted values}
$$
  \hat{\bm{y}} = \bm{X}\hat{\bm{\beta}} = \bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y} = \bm{H}\bm{y}
$$
\end{block}
where $\bm{H} = \bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'$ is the ``hat matrix''.

\pause

\begin{alertblock}{Theorem: Leave-one-out residuals}
Let $h_1,\dots,h_n$ be the diagonal values of $\bm{H}$. Then
$e_{(i)} = e_i/(1-h_i)$ is the prediction error that would be obtained if the $i$th observation was omitted.
\end{alertblock}

\pause

\begin{block}{Leverage values}
$h_i$ is called the "leverage" of observation $i$.
\end{block}

**In R:** `hatvalues(fit)`

## LOO Residuals

Let $\bm{X}_{[i]}$ and $\bm{Y}_{[i]}$ be similar to $\bm{X}$ and $\bm{Y}$ but with the $i$th row deleted in each case. Let $\bm{x}'_i$ be the $i$th row of $\bm{X}$ and let
$$
\hat{\bm{\beta}}_{[i]} = (\bm{X}_{[i]}'\bm{X}_{[i]})^{-1}\bm{X}_{[i]}' \bm{Y}_{[i]}
$$
be the estimate of $\bm{\beta}$ without the $i$th case. Then $e_{[i]} = y_i - \bm{x}_i'\hat{\bm{\beta}}_{[i]}$.

Now $\bm{X}_{[i]}'\bm{X}_{[i]} = (\bm{X}'\bm{X} - \bm{x}_i\bm{x}_i')$ and $\bm{x}_i'(\bm{X}'\bm{X})^{-1}\bm{x}_i = h_i$.

## LOO Residuals

\begin{block}{Sherman-Morrison-Woodbury formula}
Suppose $\bm{A}$ is a square matrix, and $\bm{u}$ and $\bm{v}$ are column vectors of the same dimension. Then
$$(\bm{A}+\bm{u}\bm{v}')^{-1} = \bm{A}^{-1} -
  \frac{\bm{A}^{-1}\bm{u}\bm{v}' \bm{A}^{-1}}
       {1 + \bm{v}' \bm{A}^{-1}\bm{u}}.
$$
\end{block}

So by SMW,
$$
(\bm{X}_{[i]}'\bm{X}_{[i]})^{-1} = (\bm{X}'\bm{X})^{-1} + \frac{(\bm{X}'\bm{X})^{-1}\bm{x}_i\bm{x}_i'(\bm{X}'\bm{X})^{-1}}{1-h_i}.
$$
Also note that $\bm{X}_{[i]}' \bm{Y}_{[i]} = \bm{X}'\bm{Y} - \bm{x}y_i$.

## LOO Residuals

Therefore
\begin{align*}
\bm{\hat{\beta}}_{[i]}
&=  \left[ (\bm{X}'\bm{X})^{-1}  + \frac{ (\bm{X}'\bm{X})^{-1}\bm{x}_i\bm{x}_i'(\bm{X}'\bm{X})^{-1} }{1-h_i} \right] (\bm{X}'\bm{Y} - \bm{x}_i y_i)\\\\
&=  \hat{\bm{\beta}} - \left[ \frac{ (\bm{X}'\bm{X})^{-1}\bm{x}_i}{1-h_i}\right] \left[y_i(1-h_i) -  \bm{x}_i' \hat{\bm{\beta}} +h_i y_i \right]\\\\
&=  \hat{\bm{\beta}} - (\bm{X}'\bm{X})^{-1}\bm{x}_i e_i / (1-h_i)
\end{align*}

## LOO Residuals

Thus
\begin{align*}
e_{[i]} &= y_i - \bm{x}_i'\hat{\bm{\beta}}_{[i]} \\
& = y_i - \bm{x}_i' \left[ \hat{\bm{\beta}} - (\bm{X}'\bm{X})^{-1}\bm{x}_ie_i/(1-h_i)\right] \\
&= e_i + h_i e_i/(1-h_i) \\
&= e_i/(1-h_i),
\end{align*}

## LOO Residuals

### Cross-validation statistic
$$
\text{CV} = \frac1T\sum_{i=1}^n[e_i/(1-h_i)]^2,
$$

 * Measures MSE of out-of-sample prediction
 * Asymptotically equivalent to AIC (up to monotonic transformation)

### Cook distances
$$
 D_i = \frac{e_i^2h_i}{\hat{\sigma}^2p(1-h_i)}
$$

 * Measures change in fit if observation $i$ dropped.

## Residual diagnostic functions

 * `dfbeta`: $\hat{\bm{\beta}} - \hat{\bm{\beta}}_{(i)}$
 * `hatvalues`: $h_i$
 * `cooks.distance`: $D_i$
 * `influence.measures`: $\hat{\bm{\beta}} - \hat{\bm{\beta}}_{(i)}$, $D_i$, $h_i$
 * `autoplot`
 * `car::crPlots`: component+residual plots
 * `car::residualPlots`: residuals vs variables
 * `influenceIndexPlot`: influence plots
 * `qqPlot`: Alternative QQ-plot of residuals
 * `termplot`: replaces `crPlots` for models with interactions

## Variable selection

* `step` will minimize AIC using backwards selection
* `R330::allpossregs` will fit all possible regressions
* Do not use coefficient t-tests for variable selection
* Beware of statistical tests after variable selection
* Hierarchy principle: don't eliminate lower terms while retaining higher order terms.

## Polynomial regression

**Simple approach:** Add $x$, $x^2$, $x^3$, \dots to model.

```r
fit <- lm(y ~ x + I(x^2) + I(x^3))
```

**Problems:**

  - predictors are highly correlated, so difficult to separate effects of terms.
  - numerical instability of coefficients

## Polynomial regression

```{r}
x <- seq(0,1, l=1001)
X <- cbind(x=x, `x^2`=x^2, `x^3`=x^3)
pairs(X)
```

\pause

**Solution: orthogonal polynomials**

## Orthogonal polynomials
\vspace*{-1cm}

\begin{align*}
z_1 &= a_1 + b_1 x \\
z_2 &= a_2 + b_2 x + c_2 x^2 \\
z_3 &= a_3 + b_3 x + c_3 x^2 + d_3 x^3
\end{align*}
where coefficients such that $z_i'z_j=0$ when $i\ne j$.

```r
fit <- lm(y ~ poly(x,3))
```

* Because $z_i$ involve constants $a_1, a_2, \dots$, the intercept will be affected.
* Coefficients of lower order terms are unchanged when higher order terms are added.

## Orthogonal polynomials

```{r}
x <- seq(0,1, l=1001)
Z <- poly(x,3)
colnames(Z) <- c("z1","z2","z3")
pairs(Z)
```

