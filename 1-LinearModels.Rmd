---
title: "ETC3580: Advanced Statistical Modelling"
author: "Week 1: Linear model revision"
fontsize: 14pt
output:
  beamer_presentation:
    theme: metropolis
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE,
  dev.args=list(bg=grey(0.9), pointsize=11))
library(fpp2)
```

# Unit information

## Contact details

### Lecturer

**Professor Rob Hyndman**

  - Room E762, Menzies Building
  - Email: Rob.Hyndman@monash.edu
  - robjhyndman.com

### Tutor

**Mitchell O'Hara-Wild**

  - Email: Mitch.OHaraWild@monash.edu

## Unit objectives

  1. provide an understanding of statistical models for handling common data analysis problems
  2. develop skills for fitting, interpreting and assessing statistical models
  3. develop computer skills for exploring and modelling different kinds of data.


### Teaching and learning approach

 * Two 60 min lectures (Mon 4pm; Tues 4pm)
 * One 90 min tutorial (Fri 12.30pm)


##
\placefig{2.5}{1.}{width=7.5cm}{RStudio-Ball}

## Key reference

\placefig{0}{1.2}{width=7cm}{ELM}

\begin{textblock}{6}(6,2)
\begin{alertblock}{}\Large
\centerline{\bf pdf on ed}
\end{alertblock}
\end{textblock}

## Outline

\fontsize{12}{13}\sf

\begin{tabular}{rlp{8.2cm}}
\bf Week & \multicolumn{2}{l}{\textbf{Topic}} \\
\midrule
1 &GLM & Review of linear models\\
2 &GLM & Regression with binary, binomial and proportional responses\\
3 &GLM & Regression with count responses\\
4 &GLM & Generalized linear model theory\\
5 &GLMM & Random effects\\
6 &GLMM & Panel data and hierarchical linear models\\
7 &GLMM & Mixed effects models for non-Gaussian responses\\
8 &GAM & Nonparametric regression\\
9 &GAM & Smoothing splines and B-splines\\
10 &GAM  & Penalized regression splines\\
11 &GAM  & Additive models\\
12 &GAM  & GAMM and MARS
\end{tabular}

## Assessment

\begin{tabular}{lll}
  \toprule
    \textbf{Task}  & \textbf{Due Date}     & \textbf{Value} \\
  \midrule
    Assignment 1   & 28 August & 13.33\%\\
    Assignment 2   & 2 October & 13.33\%\\
    Assignment 3   & 23 October & 13.33\%\\
    Final exam     & Official exam period  & 60\%           \\
  \bottomrule
\end{tabular}

\vspace*{0.8cm}

\pause

  - Need at least 50\% for exam.
  - Need at least 50\% for total.

# Linear Models in R

## Linear Models
\fontsize{13}{13}\sf
\begin{block}{}\centering
$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_{p-1}x_{p-1} + \varepsilon$
\end{block}

 * Response: $y$
 * Predictors: $x_1,\dots,x_{p-1}$
 * Error: $\varepsilon$

Let $\bm{y} = (y_1,\dots,y_n)'$, $\bm{\varepsilon} = (\varepsilon_1,\dots,\varepsilon_n)'$, $\bm{\beta} = (\beta_0,\dots,\beta_{p-1})'$ and
\[
\bm{X} = \begin{bmatrix}
1      & x_{1,1} & x_{2,1} & \dots & x_{p-1,1}\\
1      & x_{1,2} & x_{2,2} & \dots & x_{p-1,2}\\
\vdots & \vdots  & \vdots  &       & \vdots\\
1      & x_{1,n} & x_{2,n} & \dots & x_{p-1,n}
  \end{bmatrix} \qquad \text{(the model matrix).}
\]\pause

Then

###
\centering
$\bm{y} = \bm{X}\bm{\beta} + \bm{\varepsilon}.$

## Matrix formulation

**Least squares estimation**

Minimize: $(\bm{y} - \bm{X}\bm{\beta})'(\bm{y} - \bm{X}\bm{\beta})$\pause

Differentiate wrt $\bm{\beta}$ gives

\begin{block}{The ``normal'' equations}
\[
\hat{\bm{\beta}} = (\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y}
\]
\end{block}



## Likelihood

If the errors are iid and normally distributed, then
\[
\bm{y} \sim \text{N}(\bm{X}\bm{\beta},\sigma^2\bm{I}).
\]\pause
So the likelihood is
\[
L = \frac{1}{\sigma^T(2\pi)^{T/2}}\exp\left(-\frac1{2\sigma^2}(\bm{y}-\bm{X}\bm{\beta})'(\bm{y}-\bm{X}\bm{\beta})\right)
\]\pause
which is maximized when $(\bm{y}-\bm{X}\bm{\beta})'(\bm{y}-\bm{X}\bm{\beta})$ is minimized.\pause

\centerline{\textcolor{orange}{So \textbf{MLE = OLS}.}}




## R modelling notation

```r
fit <- lm(response ~ x1 + x2 + x3,
  data=tibble)
```

\begin{block}{}
$$\text{\tt response} = \beta_0 + \beta_1 \text{\tt x1}
 + \beta_2 \text{\tt x2}
 + \beta_3 \text{\tt x3}
 + \varepsilon$$
\end{block}

## Useful helper functions
 * `summary`
 * `coef`
 * `fitted`
 * `predict`
 * `residuals`
 * `deviance`
 * `df.residual`
 * `plot`

## R formulas

\fontsize{13}{14}\sf

\alert{Categorical predictors}:

 * R will create the required dummy variables from a categorical *factor*.
 * The first level is used as the reference category.
 * Use `relevel` to change the reference category


\fontsize{12}{12}\sf

| Expression | Description |
| :---------- |:-----------|
| `y ~ x` | Simple regression|
| `y ~ 1 + x` | Explicit intercept|
| `y ~ -1 + x`  | Through the origin|
| `y ~ x + I(x^2)`  | Quadratic regression|
| `y ~ x1 + x2 + x3`  | Multiple regression|
| `sqrt(y) ~ x + I(x^2)`  | Transformed|
| `y ~ . -x1` | All variables except x1|



## Interactions

\fontsize{13}{14}\sf
\alert{Interactions}:

 * Interactions are obtained by multiplying the relevant columns of the model matrix.
 * Use `a:b` for the interaction between `a` and `b`.
 * Use `a*b` to mean `a + b + a:b`

\alert{Limited order interactions}:

  * Interactions up to 2nd order can be specified using the `^` operator.
  * `(a+b+c)^2` is identical to `(a+b+c)*(a+b+c)`

\alert{Nested factors}:

 * `a + b %in% a`   expands to `a + a:b`.

## Interpretation

* Each coefficient gives the effect of a one unit increase of the predictor on the response variable, *holding all other variables constant*.
* Be careful with interactions: you cannot interpret the main effects when there is an interaction between them.

## Hypothesis testing

* Use F-tests between models:

    -  Model 1: $p_1$ parameters.
    -  Model 2 (nested within Model 1): $p_2$ parameters

\begin{block}{}
$$F = \frac{(\text{RSS}_2 - \text{RSS}_1)/(p_1-p_2)}
           {\text{RSS}_1/(n-p_1)}
    \sim F_{p_1-p_2, n-p_1}
$$
\end{block}

* Helper functions: `anova`, `drop1`
* If one term dropped, this is equivalent to a t-test on coefficient.

# Regression diagnostics

## Regression diagnostics

 * Residuals vs Fitted: Check heteroskedasticity
 * Scale-Location: standardized residuals vs fitted values.
 * Normal QQ plot: Check for non-normality
 * Residuals vs Leverage: Check for influential points

Produced using `autoplot` or `plot`

## LOO Residuals

\begin{block}{Fitted values}
$$
  \hat{\bm{y}} = \bm{X}\hat{\bm{\beta}} = \bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y} = \bm{H}\bm{y}
$$
\end{block}
where $\bm{H} = \bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'$ is the ``hat matrix''.

\pause

\begin{alertblock}{Theorem: Leave-one-out residuals}
Let $h_1,\dots,h_n$ be the diagonal values of $\bm{H}$. Then
$e_{(i)} = e_i/(1-h_i)$ is the prediction error that would be obtained if the $i$th observation was omitted.
\end{alertblock}

\pause

\begin{block}{Leverage values}
$h_i$ is called the "leverage" of observation $i$.
\end{block}

**In R:** `hatvalues(fit)`

## LOO Residuals

Let $\bm{X}_{[i]}$ and $\bm{Y}_{[i]}$ be similar to $\bm{X}$ and $\bm{Y}$ but with the $i$th row deleted in each case. Let $\bm{x}'_i$ be the $i$th row of $\bm{X}$ and let
$$
\hat{\bm{\beta}}_{[i]} = (\bm{X}_{[i]}'\bm{X}_{[i]})^{-1}\bm{X}_{[i]}' \bm{Y}_{[i]}
$$
be the estimate of $\bm{\beta}$ without the $i$th case. Then $e_{[i]} = y_i - \bm{x}_i'\hat{\bm{\beta}}_{[i]}$.

Now $\bm{X}_{[i]}'\bm{X}_{[i]} = (\bm{X}'\bm{X} - \bm{x}_i\bm{x}_i')$ and $\bm{x}_i'(\bm{X}'\bm{X})^{-1}\bm{x}_i = h_i$.

## LOO Residuals


\begin{block}{Sherman-Morrison-Woodbury formula}
Suppose $\bm{A}$ is a square matrix, and $\bm{u}$ and $\bm{v}$ are column vectors of the same dimension. Then
$$(\bm{A}+\bm{u}\bm{v}')^{-1} = \bm{A}^{-1} -
  \frac{\bm{A}^{-1}\bm{u}\bm{v}' \bm{A}^{-1}}
       {1 + \bm{v}' \bm{A}^{-1}\bm{u}}.
$$
\end{block}

So by SMW,
$$
(\bm{X}_{[i]}'\bm{X}_{[i]})^{-1} = (\bm{X}'\bm{X})^{-1} + \frac{(\bm{X}'\bm{X})^{-1}\bm{x}_i\bm{x}_i'(\bm{X}'\bm{X})^{-1}}{1-h_i}.
$$
Also note that $\bm{X}_{[i]}' \bm{Y}_{[i]} = \bm{X}'\bm{Y} - \bm{x}y_i$.

## LOO Residuals

Therefore
\begin{align*}
\bm{\hat{\beta}}_{[i]}
&=  \left[ (\bm{X}'\bm{X})^{-1}  + \frac{ (\bm{X}'\bm{X})^{-1}\bm{x}_i\bm{x}_i'(\bm{X}'\bm{X})^{-1} }{1-h_i} \right] (\bm{X}'\bm{Y} - \bm{x}_i y_i)\\\\
&=  \hat{\bm{\beta}} - \left[ \frac{ (\bm{X}'\bm{X})^{-1}\bm{x}_i}{1-h_i}\right] \left[y_i(1-h_i) -  \bm{x}_i' \hat{\bm{\beta}} +h_i y_i \right]\\\\
&=  \hat{\bm{\beta}} - (\bm{X}'\bm{X})^{-1}\bm{x}_i e_i / (1-h_i)
\end{align*}


## LOO Residuals

Thus
\begin{align*}
e_{[i]} &= y_i - \bm{x}_i'\hat{\bm{\beta}}_{[i]} \\
& = y_i - \bm{x}_i' \left[ \hat{\bm{\beta}} - (\bm{X}'\bm{X})^{-1}\bm{x}_ie_i/(1-h_i)\right] \\
&= e_i + h_i e_i/(1-h_i) \\
&= e_i/(1-h_i),
\end{align*}


## LOO Residuals

### Cross-validation statistic
$$
\text{CV} = \frac1T\sum_{i=1}^n[e_i/(1-h_i)]^2,
$$

 * Measures MSE of out-of-sample prediction
 * Asymptotically equivalent to AIC (up to monotonic transformation)


### Cook distances
$$
 D_i = \frac{e_i^2h_i}{\hat{\sigma}^2p(1-h_i)}
$$

 * Measures change in fit if observation $i$ dropped.


## Residual diagnostic functions

 * `dfbeta`: $\hat{\bm{\beta}} - \hat{\bm{\beta}}_{(i)}$
 * `hatvalues`: $h_i$
 * `cooks.distance`: $D_i$
 * `influence.measures`: $\hat{\bm{\beta}} - \hat{\bm{\beta}}_{(i)}$, $D_i$, $h_i$
 * `autoplot`
 * `car::crPlots`: component+residual plots
 * `car::residualPlots`: residuals vs variables
 * `influenceIndexPlot`: influence plots
 * `qqPlot`: Alternative QQ-plot of residuals
 * `termplot`: replaces `crPlots` for models with interactions

## Variable selection

* `step` will minimize AIC using backwards selection
* `R330::allpossregs` will fit all possible regressions
* Do not use coefficient t-tests for variable selection
* Beware of statistical tests after variable selection
* Hierarchy principle: don't eliminate lower terms while retaining higher order terms.

## Polynomial regression

**Simple approach:** Add $x$, $x^2$, $x^3$, \dots to model.

```r
fit <- lm(y ~ x + I(x^2) + I(x^3))
```

**Problems:**

  - predictors are highly correlated, so difficult to separate effects of terms.
  - numerical instability of coefficients

## Polynomial regression

```{r}
x <- seq(0,1, l=1001)
X <- cbind(x=x, `x^2`=x^2, `x^3`=x^3)
pairs(X)
```

\pause

**Solution: orthogonal polynomials**

## Orthogonal polynomials
\vspace*{-1cm}

\begin{align*}
z_1 &= a_1 + b_1 x \\
z_2 &= a_2 + b_2 x + c_2 x^2 \\
z_3 &= a_3 + b_3 x + c_3 x^2 + d_3 x^3
\end{align*}
where coefficients such that $z_i'z_j=0$ when $i\ne j$.

```r
fit <- lm(y ~ poly(x,3))
```

* Because $z_i$ involve constants $a_1, a_2, \dots$, the intercept will be affected.
* Coefficients of lower order terms are unchanged when higher order terms are added.


## Orthogonal polynomials


```{r}
x <- seq(0,1, l=1001)
Z <- poly(x,3)
colnames(Z) <- c("z1","z2","z3")
pairs(Z)
```

