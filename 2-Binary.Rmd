---
title: "ETC3580: Advanced Statistical Modelling"
author: "Week 2: Binary, binomial and proportion responses"
fontsize: 14pt
output:
  beamer_presentation:
    theme: metropolis
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE,
  dev.args=list(bg=grey(0.9), pointsize=11))
library(faraway)
library(ggplot2)
```

# Logistic regression

## Logistic regression

Suppose response variable $Y_i$ takes values 0 or 1 with probability $P(Y_i=1)=p_i$. (i.e., a Bernoulli distribution)

We relate $p_i$ to the predictors:
\begin{block}{}\vspace*{-0.2cm}
\begin{align*}
 p_i &= e^{\eta_i} / (1+e^{\eta_i}) = P(Y_i=1)\\
 \eta_i &= \beta_0 + \beta_1 x_{i,1} + \dots + \beta_q x_{i,q}
\end{align*}
\end{block}

* $g(p) = \log(p/(1-p))$ is the "logit" function. It maps $(0,1) \rightarrow \mathbb{R}$.
* The inverse logit is $g^{-1}(\eta) = e^\eta/(1+e^\eta)$ which maps $\mathbb{R} \rightarrow (0,1)$.
* $g$ is called the "link" function.

## Inverse logit function

```{r, fig.height=4.4}
curve(ilogit(x), -6, 6, xlab=expression(eta), ylab='p')
```

 * If $p_i\approx 0.5$, logistic and linear regression similar.

## Log-likelihood
\fontsize{13}{14}\sf
\vspace*{-0.7cm}
\begin{align*}
\log L(\bm{\beta})
 &= \sum_{i=1}^n \log(p_i)1_{Y_i=1} + \log(1-p_i)1_{Y_i=0} \\
 &= \sum_{i=1}^n y_i[\eta_i - \log(1+e^{\eta_i})] + (1-y_i)\log\left(1-\frac{e^{\eta_i}}{1+e^{\eta_i}}\right) \\
 &= \sum_{i=1}^n y_i[\eta_i - \log(1+e^{\eta_i})] + (1-y_i)\log\left(\frac{1}{1+e^{\eta_i}}\right) \\
 &= \sum_{i=1}^n y_i[\eta_i - \log(1+e^{\eta_i})] - (1-y_i)\log(1+e^{\eta_i}) \\
 &= \sum_{i=1}^n [y_i\eta_i - \log(1+e^{\eta_i})]
\end{align*}\pause

* Uses MLE to obtain $\hat{\bm{\beta}}$
* Equivalent to iterative weighted least squares
* No closed form expressions

## Logistic regression in R

```r
fit <- glm(y ~ x1 + x2, family=binomial,
            data=df)
```
* Bernoulli is equivalent to Binomial with only two levels.
* First (alphabetical) level is set to 0, other to 1. Use `relevel` if you want to change it.
* `glm` uses MLE with a logit link function (when `family=binomial`).

## Interpreting a logistic regression

\begin{block}{}
Odds = $p/(1-p)$.
\end{block}

 * Example: 3-1 odds means $p=3/4$.
 * Example: 5-2 odds means $p=5/7$.
 * A horse at "15-1 against" means $p=1/16$.
 * Odds are unbounded.
 * log(odds) = $\log(p/(1-p)) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$
 * A unit increase in $x_1$ with $x_2$ held fixed increases log-odds of success by $\beta_1$.
 * A unit increase in $x_1$ with $x_2$ held fixed increases odds of success by a factor of $e^{\beta_1}$.

# Diagnostics for logistic regression

## OLS Residuals

\alert{Response residuals}: Observation -- estimate
\begin{block}{}
\centerline{$e_i = y_i - \hat{y}_i$}
\end{block}
\pause

\alert{Pearson residuals}: Standardized
\begin{block}{}
\centerline{$r_i = e_i/\hat{\sigma}$}
\end{block}\pause\vspace*{-0.3cm}

 * Mean 0, variance 1.\pause

\alert{Deviance residuals}: Signed root contribution to \rlap{$-2\log L$.}
$$-2\log L = c + \frac{1}{\hat\sigma^2} \sum e_i^2 = c + \sum d_i^2$$
\begin{block}{}
\centerline{$d_i=e_i/\hat{\sigma}$}
\end{block}

## Logistic regression residuals

\alert{Response residuals}: Observation -- estimate
\begin{block}{}
\centerline{$e_i = y_i - \hat{p}_i$}
\end{block}
\pause

\alert{Pearson residuals}: Standardized
\begin{block}{}
\centerline{$r_i = \frac{y_i - \hat{p}_i}{\sqrt{\hat{p}_i(1-\hat{p}_i)}}$}
\end{block}
\pause

 * Mean 0, variance 1.
\pause

\alert{Deviance residuals}:  Signed root contribution to \rlap{$-2\log L$.}
$$ -2\log L = -2\sum [\log(\hat{p}_i)y_i + \log(1-\hat{p}_i)(1-y_i)]$$
\begin{block}{}
\centerline{$d_i = (2y_i-1)\sqrt{-2\left[y_i\log(\hat{p}_i) + (1-y_i)\log(1-\hat{p}_i)\right]}$}
\end{block}

## Logistic regression residuals

In R:
```r
fit <- glm(y ~ x1 + x2, family='binomial',
         data=df)
e <- residuals(fit, type='response')
r <- residuals(fit, type='pearson')
d <- residuals(fit, type='deviance')
```

* `deviance` is the default
* Residual plots can be hard to interpret
* Don't expect residuals to be normally distributed

## Partial residual plots

* Let $e_i = y_i-\hat{p}_i$ be a response residual. Then 
$$e_i^* = \frac{e_i}{\hat{p}_i(1-\hat{p}_i)}$$
is the "logit residual" (compare Pearson residuals).

* The "logit partial residual" for the $j$th variable is
$$e_{i,j}^* + \hat{\beta}_j x_{i,j}$$

* We can plot these against $x_j$ to identify potential nonlinearity.

* `car::crPlots` implements them

# Inference for logistic regression

## Deviance
\fontsize{14}{14}\sf

Generalization of sum of squared residuals based on likelihood ratio:
$$D = \sum d_i^2 = -2\log L + c$$
where $L$ is the likelihood of the model and $c$ is constant that depends on data but not model.

 * Difference between deviances equivalent to a likelihood ratio test.
 * $D_1-D_2 \sim \chi^2_{q_2-q_1}$ where $q_i$ is df for model $i$ assuming
     1. smaller model is correct
     2. models are nested
     3. distributional assumptions true
 * Null deviance is for model with only an intercept.

## Deviance test

In R:

```r
fit <- glm(y ~ x1 + x2, family='binomial',
         data=df)
anova(fit, test="Chisq")
drop1(fit, test="Chisq")
anova(fit1, fit2, test="Chisq")
```

 * NOT equivalent to t-tests on coefficients
 * Deviance tests preferred

## Confidence intervals for coefficients
\fontsize{14}{15}\sf

 * Standard intervals based on normal distribution are poor approximations.
 * Better to use "profile likelihood" confidence intervals
 * Let $L_p(\bm{\theta})$ be the profile likelihood (the likelihood without the nuisance parameters). LR test for $H_0: \bm{\theta}=\bm{\theta}_0$ is
   $$LR = 2\left[\log L_p(\hat{\bm\theta}) - \log L_p(\bm\theta_0)\right]$$
   Confidence interval consists of those values $\bm{\theta}_0$ for which test is not significant. (A contour region of the likelihood.)
 * Implemented in R using `confint`

## Model selection

\begin{block}{Akaike's Information Criterion}
\centerline{$\text{AIC} = -2\log L + 2q = c + D + 2q$}
\end{block}

 * Select model with smallest AIC
 * Beware of hypothesis tests after variable selection

# Latent variables and link functions

## Latent variable interpretation
\fontsize{14}{16}\sf

 * Suppose $z$ is a latent (unobserved) random variable:
$$y = \begin{cases}
  1 & z = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q + \varepsilon > 0 \\
  0 & \text{otherwise}
  \end{cases}
$$
where $\varepsilon$ has cdf $F$.
* If $F$ is "standard logistic", then $F(w) = 1/[1+e^{-w}]$.
* So $\text{logit}(p) = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q$.

###
That is, we can think of logistic regression as an ordinary regression with logistic noise, and we observe only if it is above or below 0.

## Latent variable interpretation
\fontsize{14}{16}\sf

 * Suppose $z$ is a latent (unobserved) random variable:
$$y = \begin{cases}
  1 & z = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q + \varepsilon > 0 \\
  0 & \text{otherwise}
  \end{cases}
$$
where $\varepsilon$ has cdf $F$.
* If $F$ is "standard normal", then $F(w) = \Phi(w)$.
* So $\Phi^{-1}(p) = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q$.\pause
* Here $\Phi^{-1}$ is the link function.

\vspace*{10cm}

## Latent variable interetation

\begin{block}{General binary model}\vspace*{-0.2cm}
\begin{align*}
 p_i &= g(\eta_i) = P(Y_i=1)\\
 \eta_i &= \beta_0 + \beta_1 x_{i,1} + \dots + \beta_q x_{i,q}
\end{align*}
where $g$ maps $\mathbb{R}\rightarrow (0,1)$.
\end{block}

* $g(\eta) = e^\eta/(1+e^\eta)$: logit link, logistic regression
* $g(\eta) = \Phi(\eta)$: normal cdf link, probit regression
* $g(\eta) = 1-\exp(-\exp(\eta))$: log-log link

\pause

```r
fit <- glm(y ~ x1 + x2,
  family=binomial(link=probit), data=df)
```

## Why prefer logit over probit?

* odds ratio interpretation of coefficients
* non-biased with disproportionate stratified sampling. Only link function with this property.
* non-biased with clustered observations. Only link function with this property.
* logit link is "canonical": ensures $\sum x_{ij}y_i$ $(j=1,\dots,q)$ are sufficient for estimation. So $p$-values are exact.

# Binomial responses

## Binomial responses

 * $Y$ is binomially distributed $B(m,p)$ if
 $$P(Y=y) = {m \choose y} p^y (1-p)^{m-y}$$
 * $Y=$ number of "successes" in $m$ independent trials, each with probability $p$ of success.
 * Binomial regression also uses a logit link\vspace*{-0.4cm}
\begin{align*}
 p_{i} &= e^{\eta_i} / (1+e^{\eta_i})\\
 \eta_i &= \beta_0 + \beta_1 x_{i,1} + \dots + \beta_q x_{i,q}
\end{align*}\vspace*{-0.2cm}
$$\log L = \sum_{i=1}^n \left[y_i\eta_i - m_i \log(1+e^{\eta_i}) + log {m_i \choose y_i}\right]$$

## Binomial responses

In R:

 * `glm` needs a two-column matrix of success and failures. (So rows sum to $m$).

```r
fit <- glm(cbind(successes,failures) ~
    x1 + x2,
  family=binomial, data=df)
```

 * Everything else works the same as for binary regression.

## Overdisperson

 * If mean correctly modelled, but observed variance larger than model, we called the data "overdispersed". [Same for underdispersion.]
 * Concept of overdispersion irrelevant for OLS and logistic regression because there cannot be any more variance than what is modelled.
 * For binomial regression:
  $y_i \sim B(m_i,p_i)$, $\E(y_i)= m_ip_i$, $\V(y_i) = m_ip_i(1-p_i)$.
 * If model correct, $D\sim \chi^2_{n-q}$.

     So $D>n-q$ indicates overdisperson.

## Overdisperson

$D>n-q$ can also be the result of:

 * missing covariates or interaction terms
 * negligence of non-linear effects
 * large outliers
 * sampling from clusters
 * non-independence
 * $m$ small ($\chi^2$ approximation fails)

## Overdisperson
\fontsize{14}{16}\sf

\alert{Solution 1:} Drop strict binomial assumption and let
$\E(y_i)= m_ip_i$, $\V(y_i) = \phi m_ip_i(1-p_i)$.

\begin{block}{Pearson residuals}
\centerline{$\displaystyle r_i = \frac{y_i - m_i\hat{p}_i}{\sqrt{m_i\hat{p}_i(1-\hat{p}_i)}}$}
\end{block}

\begin{block}{Simple estimate of dispersion parameter}
Estimate\qquad\qquad  $\displaystyle\hat\phi = \frac1n\sum_{i=1}^n r_i^2$.
\end{block}

 * OK for estimation and standard errors on coefficients.
 * But no proper inference via deviance.

## Overdisperson

\alert{Solution 2:} Define a "quasi-likelihood" that behaves like the log-likelihood but allows for $\V(y_i) = \phi m_ip_i(1-p_i)$.

```r
fit <- glm(cbind(successes,failures) ~
    x1 + x2,
  family=quasibinomial, data=df)
```

* Must use $F$-tests rather than $\chi^2$ tests for comparing models.

\vspace*{3cm}

# Proportion responses

## Proportion responses

Suppose $y_i \in [0,1]$ are proportions. We can use a quasi-binomial model:

 * logit link keeps predicted proportions in $(0,1)$
 * Variance function $\phi p(1-p)$ makes sense for proportions as greatest variation around $p=0.5$ and least around $p=0$ and $p=1$.

```r
glm(y ~ x1 + x2,
  family=quasibinomial,
  data=df)
```

## Beta regression
\fontsize{13}{14}\sf

\begin{block}{Beta density:}
$$f(y) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} y^{a-1} (1-y)^{b-1}$$
\vspace*{-0.3cm}

where $y\in[0,1]$ and
$\Gamma(u) = \int _0^\infty x^{u-1}e^{-x} dx$.
\begin{itemize}
\item $\E(y) = \frac{a}{a+b}$ \qquad $\V(y) = \frac{ab}{(a+b)^2(a+b+1)}$
\end{itemize}
\end{block}\pause

* Reparameterize so $\mu=a/(a+b)$ and $\phi=a+b$.\pause
* Then $\E(Y)=\mu$ and $\V(Y)=\mu(1-\mu)/(1+\phi)$.\pause

\begin{block}{}\vspace*{-0.2cm}
\begin{align*}
 \mu_i &= e^{\eta_i} / (1+e^{\eta_i})\\
 \eta_i &= \beta_0 + \beta_1 x_{i,1} + \dots + \beta_q x_{i,q}
\end{align*}
\end{block}

* `mgcv::gam(y ~ x1+x2, family=betar())`
